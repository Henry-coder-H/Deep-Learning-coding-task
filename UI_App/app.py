"""
æ™ºèƒ½äº¤é€šè¯†åˆ«ç³»ç»Ÿ - Streamlit UI
é›†æˆè½¦å‹è¯†åˆ«ã€è½¦ç‰Œè¯†åˆ«ã€è½¦é€Ÿè¯†åˆ«ä¸‰å¤§æ ¸å¿ƒåŠŸèƒ½
"""
import streamlit as st
import cv2
import numpy as np
import tempfile
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import time
import pandas as pd
from PIL import Image, ImageDraw, ImageFont
import colorsys
from streamlit_image_coordinates import streamlit_image_coordinates
import matplotlib.pyplot as plt
import matplotlib
from collections import defaultdict

matplotlib.rcParams['font.sans-serif'] = ['SimHei']
matplotlib.rcParams['axes.unicode_minus'] = False

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ™ºèƒ½äº¤é€šè¯†åˆ«ç³»ç»Ÿ",
    page_icon="ğŸš—",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- æ³¨å…¥ CSS æ ·å¼æ¥â€œæ±‰åŒ–â€ä¸Šä¼ ç»„ä»¶ ---
# --- æ³¨å…¥ CSS æ ·å¼æ¥â€œæ±‰åŒ–â€ä¸Šä¼ ç»„ä»¶ (v2.0 ä¼˜åŒ–ç‰ˆ) ---
st.markdown("""
<style>
    /* 1. éšè—åŸæ¥çš„ "Drag and drop file here" å’Œ "Limit 200MB..." */
    [data-testid='stFileUploaderDropzone'] div div span,
    [data-testid='stFileUploaderDropzone'] div div small {
       display: none;
    }
    
    /* 2. è‡ªå®šä¹‰ä¸­é—´æç¤ºæ–‡æœ¬ */
    [data-testid='stFileUploaderDropzone'] div div::after {
       content: "ç‚¹å‡»ä¸Šä¼ æ–‡ä»¶æˆ–å°†æ–‡ä»¶æ‹–æ‹½è‡³æ­¤å¤„";
       font-size: 16px; /* ğŸ’¡ ä¿®æ”¹è¿™é‡Œï¼šè°ƒå°äº†å­—ä½“ (åŸæ¥æ˜¯ 1.2em) */
       margin-bottom: 10px;
    }

    /* 3. å¼ºè¡Œä¿®æ”¹ "Browse files" æŒ‰é’®æ–‡å­— */
    /* ç¬¬ä¸€æ­¥ï¼šæŠŠæŒ‰é’®é‡Œçš„åŸè‹±æ–‡å˜é€æ˜/éšè— */
    [data-testid='stFileUploaderDropzone'] button {
        font-size: 0 !important; /* å°†åŸå­—ä½“è®¾ä¸º0ï¼Œç›¸å½“äºéšè— */
        min-width: 80px; /* ä¿è¯æŒ‰é’®å®½åº¦ */
    }
    
    /* ç¬¬äºŒæ­¥ï¼šåœ¨æŒ‰é’®ä¸Šç”¨ä¼ªå…ƒç´ â€œå†™â€ä¸Šä¸­æ–‡ */
    [data-testid='stFileUploaderDropzone'] button::after {
        content: "ä¸Šä¼ æ–‡ä»¶"; /* âœ¨ è¿™é‡Œæ”¹æŒ‰é’®æ–‡å­— */
        font-size: 14px !important; /* æ¢å¤å­—ä½“å¤§å° */
        color: inherit; /* è·Ÿéšä¸»é¢˜é¢œè‰² */
        visibility: visible;
        display: block;
        padding-top: 2px;
    }
</style>
""", unsafe_allow_html=True)

# é…ç½®è·¯å¾„
WEIGHTS_DIR = Path(__file__).parent / "weights"
TEMP_DIR = Path(tempfile.gettempdir()) / "traffic_recognition"
TEMP_DIR.mkdir(parents=True, exist_ok=True)

# æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
IMAGE_EXTENSIONS = ['jpg', 'jpeg', 'png']
VIDEO_EXTENSIONS = ['mp4', 'avi', 'mov']


def get_distinct_colors(n: int) -> List[Tuple[int, int, int]]:
    """ç”Ÿæˆnä¸ªè§†è§‰ä¸Šå¯åŒºåˆ†çš„é¢œè‰²"""
    colors = []
    for i in range(n):
        hue = i / n
        saturation = 0.7 + (i % 3) * 0.1
        value = 0.8 + (i % 2) * 0.1
        rgb = colorsys.hsv_to_rgb(hue, saturation, value)
        colors.append((int(rgb[2] * 255), int(rgb[1] * 255), int(rgb[0] * 255)))  # BGR
    return colors

# é¢„ç”Ÿæˆ100ç§é¢œè‰²ç”¨äºè½¦è¾†ID
VEHICLE_COLORS = get_distinct_colors(100)


def get_vehicle_color(track_id: int) -> Tuple[int, int, int]:
    """æ ¹æ®è½¦è¾†IDè·å–å›ºå®šé¢œè‰²"""
    return VEHICLE_COLORS[track_id % len(VEHICLE_COLORS)]


def get_chinese_font(size: int = 20):
    """è·å–æ”¯æŒä¸­æ–‡çš„å­—ä½“"""
    # å¸¸è§çš„ä¸­æ–‡å­—ä½“è·¯å¾„
    font_paths = [
        "C:/Windows/Fonts/msyh.ttc",      # å¾®è½¯é›…é»‘
        "C:/Windows/Fonts/simhei.ttf",    # é»‘ä½“
        "C:/Windows/Fonts/simsun.ttc",    # å®‹ä½“
        "/usr/share/fonts/truetype/wqy/wqy-zenhei.ttc",  # Linux
        "/System/Library/Fonts/PingFang.ttc",  # macOS
    ]
    
    for font_path in font_paths:
        if os.path.exists(font_path):
            try:
                return ImageFont.truetype(font_path, size)
            except:
                continue
    
    # å¦‚æœæ‰¾ä¸åˆ°ä¸­æ–‡å­—ä½“ï¼Œè¿”å›é»˜è®¤å­—ä½“
    return ImageFont.load_default()


def put_chinese_text(img: np.ndarray, text: str, position: Tuple[int, int], 
                     font_size: int = 20, color: Tuple[int, int, int] = (255, 255, 255),
                     bg_color: Tuple[int, int, int] = None) -> np.ndarray:
    """
    åœ¨å›¾åƒä¸Šç»˜åˆ¶æ”¯æŒä¸­æ–‡çš„æ–‡æœ¬
    
    Args:
        img: BGRå›¾åƒ
        text: è¦ç»˜åˆ¶çš„æ–‡æœ¬
        position: (x, y) å·¦ä¸Šè§’ä½ç½®
        font_size: å­—ä½“å¤§å°
        color: æ–‡å­—é¢œè‰² (BGR)
        bg_color: èƒŒæ™¯é¢œè‰² (BGR), Noneè¡¨ç¤ºæ— èƒŒæ™¯
    
    Returns:
        ç»˜åˆ¶åçš„å›¾åƒ
    """
    img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(img_pil)
    font = get_chinese_font(font_size)
    
    # PILä½¿ç”¨RGBé¢œè‰²
    text_color = (color[2], color[1], color[0])
    
    # ç»˜åˆ¶èƒŒæ™¯
    if bg_color is not None:
        bg_rgb = (bg_color[2], bg_color[1], bg_color[0])
        bbox = draw.textbbox(position, text, font=font)
        padding = 3
        draw.rectangle([bbox[0] - padding, bbox[1] - padding, 
                       bbox[2] + padding, bbox[3] + padding], fill=bg_rgb)
    
    draw.text(position, text, font=font, fill=text_color)
    
    return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)


# app.py

# 1. åŸæœ‰çš„ Torch æ¨¡å‹åŠ è½½å™¨ (ç»™å›¾ç‰‡ç”¨)
@st.cache_resource
def load_torch_plate_recognizer():
    """åŠ è½½åŸç‰ˆ PyTorch è½¦ç‰Œè¯†åˆ«æ¨¡å‹ (ç”¨äºå›¾ç‰‡)"""
    from models.plate_recognizer import PlateRecognizer  # ä½ çš„æ—§æ¨¡å‹æ–‡ä»¶
    
    det_weights = WEIGHTS_DIR / "plate_detect.pt"
    rec_weights = WEIGHTS_DIR / "plate_rec.pth"
    
    # æ£€æŸ¥æƒé‡æ˜¯å¦å­˜åœ¨
    if not det_weights.exists() or not rec_weights.exists():
        # å¦‚æœæ˜¯å›¾ç‰‡æ¨¡å¼æŠ¥é”™ï¼Œæˆ‘ä»¬åªåœ¨æ—¥å¿—é‡Œæç¤ºï¼Œä¸é˜»æ–­è§†é¢‘åŠŸèƒ½
        print(f"Torchæƒé‡ç¼ºå¤±: {det_weights} æˆ– {rec_weights}")
        return None
        
    try:
        device = "cuda:0" if is_cuda_available() else "cpu"
        return PlateRecognizer(str(det_weights), str(rec_weights), device=device)
    except Exception as e:
        st.error(f"åŠ è½½ Torch è½¦ç‰Œæ¨¡å‹å¤±è´¥: {e}")
        return None


@st.cache_resource
def load_paddle_plate_recognizer():
    """åŠ è½½ Paddle è§†é¢‘ä¸“ç”¨è½¦ç‰Œè¯†åˆ«æ¨¡å‹ (åŸºäº test.py)"""
    from models.paddle_model import PaddleVideoRecognizer
    
    # ç¡®ä¿ yolov8n.pt åœ¨ weights ç›®å½•ä¸‹
    yolo_weights = WEIGHTS_DIR / "yolov8n.pt"
    
    if not yolo_weights.exists():
        st.error(f"âŒ è§†é¢‘è¯†åˆ«éœ€è¦ yolov8n.ptï¼Œè¯·æ£€æŸ¥ {WEIGHTS_DIR}")
        return None
        
    try:
        import torch
        use_gpu = torch.cuda.is_available()
        return PaddleVideoRecognizer(str(yolo_weights), use_gpu=use_gpu)
    except Exception as e:
        st.error(f"åŠ è½½ Paddle è§†é¢‘æ¨¡å‹å¤±è´¥: {e}")
        return None

@st.cache_resource
def load_speed_estimator():
    """åŠ è½½è½¦é€Ÿä¼°è®¡å™¨ (ç¼“å­˜)"""
    from models.speed_estimator import VehicleSpeedEstimator
    
    vehicle_weights = WEIGHTS_DIR / "yolov11l.pt"
    
    if not vehicle_weights.exists():
        st.error(f"âŒ è½¦è¾†æ£€æµ‹æ¨¡å‹æƒé‡ä¸å­˜åœ¨ï¼è¯·å°† yolov11l.pt æ”¾ç½®åœ¨ {WEIGHTS_DIR} ç›®å½•ä¸‹")
        return None
        
    try:
        return VehicleSpeedEstimator(fps=30.0, vehicle_model_path=str(vehicle_weights))
    except Exception as e:
        st.error(f"åŠ è½½è½¦é€Ÿä¼°è®¡å™¨å¤±è´¥: {e}")
        return None


@st.cache_resource
def load_vehicle_classifier():
    """åŠ è½½è½¦å‹åˆ†ç±»å™¨ (ç¼“å­˜)"""
    from models.vehicle_classifier import VehicleTypeClassifier
    
    # æŒ‡å‘ä½ çš„ Task1 è®­ç»ƒå¥½çš„æƒé‡
    # è¯·ç¡®ä¿å°† task1/best.pt å¤åˆ¶åˆ° UI_App/weights/best.pt
    classifier_weights = WEIGHTS_DIR / "best.pt" 
    
    if classifier_weights.exists():
        return VehicleTypeClassifier(str(classifier_weights))
    else:
        st.warning(f"âš ï¸ æœªæ‰¾åˆ°è½¦å‹è¯†åˆ«æƒé‡: {classifier_weights}ï¼Œè¯·ä¸Šä¼ æ–‡ä»¶ã€‚")
        return VehicleTypeClassifier() # ç©ºæ¨¡å‹ï¼Œé˜²æ­¢æŠ¥é”™


def is_cuda_available() -> bool:
    """æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨"""
    try:
        import torch
        return torch.cuda.is_available()
    except:
        return False


def is_image_file(filename: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºå›¾ç‰‡æ–‡ä»¶"""
    ext = filename.lower().split('.')[-1]
    return ext in IMAGE_EXTENSIONS


def is_video_file(filename: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶"""
    ext = filename.lower().split('.')[-1]
    return ext in VIDEO_EXTENSIONS

def associate_plates_to_vehicles(vehicles: List[Dict], plates: List[Dict]) -> List[Dict]:
    """
    å°†è½¦ç‰Œæ£€æµ‹ç»“æœåˆ†é…ç»™è½¦è¾†æ£€æµ‹ç»“æœ (åŸºäºä¸­å¿ƒç‚¹åŒ…å«å…³ç³»)
    
    Args:
        vehicles: è½¦è¾†æ£€æµ‹ç»“æœåˆ—è¡¨ (éœ€åŒ…å« bbox å­—æ®µ)
        plates: è½¦ç‰Œæ£€æµ‹ç»“æœåˆ—è¡¨ (éœ€åŒ…å« bbox, text, conf å­—æ®µ)
    
    Returns:
        åˆå¹¶åçš„æ£€æµ‹ç»“æœåˆ—è¡¨
    """
    if not vehicles:
        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°è½¦ï¼ŒæŠŠæ‰€æœ‰è½¦ç‰Œä½œä¸ºç‹¬ç«‹å¯¹è±¡è¿”å›
        return [{'bbox': p['bbox'], 'plate_text': p['text'], 'plate_conf': p['conf'], 'track_id': -1} for p in plates]
    
    # 1. æ·±æ‹·è´è½¦è¾†åˆ—è¡¨ï¼Œä½œä¸ºæœ€ç»ˆç»“æœçš„åŸºç¡€
    # æ³¨æ„ï¼šæˆ‘ä»¬è¦ä¿ç•™åŸå§‹çš„ vehicle å­—å…¸ç»“æ„
    merged_results = [v.copy() for v in vehicles]
    
    # 2. éå†æ‰€æœ‰è½¦ç‰Œï¼Œå°è¯•åŒ¹é…è½¦è¾†
    for plate in plates:
        px1, py1, px2, py2 = plate.get('bbox')
        p_cx = (px1 + px2) / 2
        p_cy = (py1 + py2) / 2
        
        matched = False
        for vehicle in merged_results:
            vx1, vy1, vx2, vy2 = vehicle['bbox']
            
            # æ ¸å¿ƒé€»è¾‘ï¼šåˆ¤å®šè½¦ç‰Œä¸­å¿ƒæ˜¯å¦åœ¨è½¦è¾†æ¡†å†…
            if vx1 < p_cx < vx2 and vy1 < p_cy < vy2:
                # åŒ¹é…æˆåŠŸï¼å°†è½¦ç‰Œä¿¡æ¯æ³¨å…¥åˆ°è¯¥è½¦è¾†å­—å…¸ä¸­
                vehicle['plate_text'] = plate.get('text', '')
                vehicle['plate_conf'] = plate.get('conf', 0)
                # æ ‡è®°å·²åŒ¹é…
                matched = True
                break # ä¸€ä¸ªè½¦ç‰Œåªèƒ½å½’å±ä¸€è¾†è½¦ï¼Œæ‰¾åˆ°åè·³å‡º
        
        # 3. å¤„ç†æœªåŒ¹é…çš„å­¤ç«‹è½¦ç‰Œ (ä¾‹å¦‚è½¦æ²¡è¯†åˆ«å‡ºæ¥ï¼Œä½†è¯†åˆ«åˆ°äº†ç‰Œ)
        if not matched:
            merged_results.append({
                'bbox': plate.get('bbox'),
                'plate_text': plate.get('text', ''),
                'plate_conf': plate.get('conf', 0),
                'track_id': -1, # å­¤ç«‹è½¦ç‰Œæ²¡æœ‰è½¦è¾†ID
                'vehicle_type': 'Unknown', # å¯é€‰
                'conf': 0.0
            })
            
    return merged_results

def draw_detection_results(image: np.ndarray, detections: List[Dict], 
                           show_plate: bool = True, show_type: bool = True,
                           show_speed: bool = False) -> np.ndarray:
    """
    åœ¨å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœï¼ˆæ”¯æŒä¸­æ–‡æ˜¾ç¤ºï¼‰
    
    Args:
        image: è¾“å…¥å›¾åƒ (BGR)
        detections: æ£€æµ‹ç»“æœåˆ—è¡¨
        show_plate: æ˜¯å¦æ˜¾ç¤ºè½¦ç‰Œ
        show_type: æ˜¯å¦æ˜¾ç¤ºè½¦å‹
        show_speed: æ˜¯å¦æ˜¾ç¤ºè½¦é€Ÿ
    """
    vis = image.copy()
    
    for det in detections:
        track_id = det.get('track_id', 0)
        bbox = det.get('bbox', None)
        
        if bbox is None:
            continue
            
        x1, y1, x2, y2 = [int(v) for v in bbox]
        
        # æ ¹æ®è½¦è¾†IDåˆ†é…é¢œè‰²
        color = get_vehicle_color(track_id)
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        cv2.rectangle(vis, (x1, y1), (x2, y2), color, 2)
        
        # æ„å»ºæ ‡ç­¾æ–‡æœ¬
        labels = []
        if show_type and 'vehicle_type' in det:
            labels.append(det['vehicle_type'])
        if show_plate and 'plate_text' in det:
            labels.append(det['plate_text'])
        if show_speed and 'speed' in det:
            labels.append(f"{det['speed']:.0f}km/h")
            
        if labels:
            label_text = " | ".join(labels)
            
            # è®¡ç®—æ–‡æœ¬ä½ç½®
            text_y = y1 - 5 if y1 > 30 else y2 + 25
            
            # ä½¿ç”¨æ”¯æŒä¸­æ–‡çš„ç»˜åˆ¶å‡½æ•°
            vis = put_chinese_text(vis, label_text, (x1, text_y - 20), 
                                   font_size=20, color=(255, 255, 255), bg_color=color)
            
    return vis

def draw_statistics_charts(vehicle_counts, time_series_data=None):
    """
    ç»˜åˆ¶ç»Ÿè®¡å›¾è¡¨
    Args:
        vehicle_counts: dict, {è½¦å‹: æ•°é‡}
        time_series_data: list, [{'time': t, 'count': c}, ...] (ä»…è§†é¢‘éœ€è¦)
    """
    if not vehicle_counts:
        st.info("æš‚æ— ç»Ÿè®¡æ•°æ®")
        return

    # å‡†å¤‡æ•°æ®
    labels = list(vehicle_counts.keys())
    sizes = list(vehicle_counts.values())
    
    # é¢œè‰²æ˜ å°„ (ä¸ OpenCV ç»˜å›¾ä¿æŒä¸€è‡´ï¼Œè½¬ä¸º Hex æˆ– RGB 0-1)
    # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œä½¿ç”¨ matplotlib é»˜è®¤æˆ–è‡ªå®šä¹‰ä¸€ç»„
    
    if time_series_data is not None:
        # === è§†é¢‘æ¨¡å¼ï¼šåŒå›¾ (é¥¼å›¾ + æŠ˜çº¿å›¾) ===
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # 1. é¥¼å›¾
        ax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, shadow=True)
        ax1.set_title("è½¦å‹åˆ†å¸ƒæ¯”ä¾‹")
        
        # 2. æŠ˜çº¿å›¾
        if time_series_data:
            df_time = pd.DataFrame(time_series_data)
            ax2.plot(df_time['time'], df_time['count'], marker='o', linestyle='-', color='b', linewidth=2)
            ax2.fill_between(df_time['time'], df_time['count'], color='skyblue', alpha=0.3)
            ax2.set_xlabel("æ—¶é—´ (s)")
            ax2.set_ylabel("ç´¯è®¡è½¦è¾†æ€»æ•°")
            ax2.set_title("è½¦æµé‡éšæ—¶é—´è¶‹åŠ¿")
            ax2.grid(True, linestyle='--', alpha=0.7)
            
    else:
        # === å›¾ç‰‡æ¨¡å¼ï¼šå•å›¾ (é¥¼å›¾) ===
        fig, ax1 = plt.subplots(figsize=(6, 6))
        ax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, shadow=True)
        ax1.set_title("è½¦å‹è¯†åˆ«åˆ†å¸ƒæ¯”ä¾‹")
    
    st.pyplot(fig)
    plt.close(fig) # é‡Šæ”¾å†…å­˜

def process_image(image: np.ndarray, enable_plate: bool, enable_type: bool) -> Tuple[np.ndarray, pd.DataFrame]:
    """å¤„ç†å•å¼ å›¾ç‰‡ (å·²æ·»åŠ åˆå¹¶é€»è¾‘)"""
    vehicle_detections = []
    plate_raw_results = [] # æš‚å­˜åŸå§‹è½¦ç‰Œç»“æœ
    
    vehicle_counts = defaultdict(int) 
    
    # --- 1. è½¦å‹è¯†åˆ« (æ”¶é›†è½¦è¾†æ¡†) ---
    vehicle_classifier = load_vehicle_classifier() if enable_type else None
    if enable_type and vehicle_classifier:
        type_results = vehicle_classifier.predict(image)
        for res in type_results:
            # æ„é€ æ ‡å‡†è½¦è¾†å¯¹è±¡
            vehicle_detections.append({
                'bbox': res['bbox'],
                'vehicle_type': res['class_name'],
                'track_id': -1, 
                'conf': res['conf']
            })
            vehicle_counts[res['class_name']] += 1
            
    # --- 2. è½¦ç‰Œè¯†åˆ« (æ”¶é›†è½¦ç‰Œç»“æœ) ---
    if enable_plate:
        plate_recognizer = load_torch_plate_recognizer()
        if plate_recognizer:
            plate_raw_results = plate_recognizer.recognize_image(image)
    
    # --- 3. æ‰§è¡Œåˆå¹¶é€»è¾‘ ---
    # å¦‚æœå¼€å¯äº†è½¦å‹è¯†åˆ«ï¼Œå°è¯•å°†è½¦ç‰Œå½’å¹¶åˆ°è½¦è¾†ä¸­ï¼›å¦åˆ™ç›´æ¥æ˜¾ç¤ºè½¦ç‰Œ
    if enable_type and vehicle_detections:
        final_detections = associate_plates_to_vehicles(vehicle_detections, plate_raw_results)
    else:
        # å¦‚æœæ²¡å¼€è½¦å‹è¯†åˆ«ï¼Œæˆ–è€…æ²¡æ£€æµ‹åˆ°è½¦ï¼Œç›´æ¥è½¬æ¢è½¦ç‰Œæ ¼å¼
        final_detections = vehicle_detections # å…ˆåŒ…å«å·²æœ‰çš„(å¯èƒ½æ˜¯ç©ºçš„)
        for p in plate_raw_results:
            final_detections.append({
                'bbox': p['bbox'],
                'plate_text': p['text'],
                'plate_conf': p['conf'],
                'track_id': -1
            })

    # --- 4. ç»˜åˆ¶ç»“æœ ---
    # draw_detection_results ä¼šè‡ªåŠ¨å¤„ç†å­—å…¸é‡ŒåŒæ—¶æœ‰ vehicle_type å’Œ plate_text çš„æƒ…å†µ
    vis_image = draw_detection_results(image, final_detections, show_plate=enable_plate, show_type=enable_type)
    
    # --- 5. ç”Ÿæˆç»Ÿè®¡è¡¨æ ¼ (ç°åœ¨ä¸€è¡Œæ•°æ®ä¼šåŒæ—¶åŒ…å«è½¦å‹å’Œè½¦ç‰Œ) ---
    table_data = []
    for det in final_detections:
        row = {}
        # åªæœ‰å½“æ£€æµ‹ç»“æœåŒ…å«ç›¸å…³ä¿¡æ¯æ—¶æ‰åŠ å…¥è¡¨æ ¼
        has_info = False
        
        if 'vehicle_type' in det:
             row['ç±»å‹'] = det['vehicle_type']
             row['ç±»å‹ç½®ä¿¡åº¦'] = f"{det['conf']:.2f}"
             has_info = True
             
        if 'plate_text' in det:
             row['è½¦ç‰Œ'] = det['plate_text']
             row['è½¦ç‰Œç½®ä¿¡åº¦'] = f"{det['plate_conf']:.2f}" # å¯é€‰
             has_info = True
        
        if has_info: 
            table_data.append(row)
        
    df = pd.DataFrame(table_data)
    
    st.session_state.temp_vehicle_counts = vehicle_counts
    st.session_state.temp_time_series = None
    
    return vis_image, df


def process_video(video_path: str, enable_plate: bool, enable_type: bool, enable_speed: bool,
                  speed_estimator=None, progress_callback=None) -> str:
    """å¤„ç†è§†é¢‘ (å·²æ·»åŠ åˆå¹¶é€»è¾‘)"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened(): raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
        
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    output_path = str(TEMP_DIR / f"output_{int(time.time())}.mp4")
    fourcc = cv2.VideoWriter_fourcc(*'avc1') 
    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    plate_recognizer = load_paddle_plate_recognizer() if enable_plate else None
    vehicle_classifier = load_vehicle_classifier() if enable_type else None
    
    if enable_speed and speed_estimator:
        speed_estimator.fps = fps
        speed_estimator.set_frame_size(width, height)
        speed_estimator.reset()
        
    unique_vehicle_ids = defaultdict(set) 
    time_series_data = []
    
    frame_idx = 0
    while True:
        ret, frame = cap.read()
        if not ret: break
        
        current_vehicles = []
        current_plates = []
        
        # --- A. è½¦å‹è¯†åˆ« (è·å–å¸¦ ID çš„è½¦è¾†) ---
        if enable_type and vehicle_classifier:
            type_results = vehicle_classifier.track(frame)
            for res in type_results:
                tid = res['track_id']
                cls_name = res['class_name']
                if tid != -1:
                    unique_vehicle_ids[cls_name].add(tid)
                
                current_vehicles.append({
                    'track_id': tid,
                    'bbox': res['bbox'],
                    'vehicle_type': cls_name,
                    'conf': res['conf']
                })

        # --- B. è½¦ç‰Œè¯†åˆ« ---
        if enable_plate and plate_recognizer:
            # è¿™é‡Œçš„ recognize_image è¿”å›çš„æ˜¯ list[dict]
            current_plates = plate_recognizer.recognize_image(frame)

        # --- C. åˆå¹¶é€»è¾‘ ---
        if enable_type and current_vehicles:
            final_detections = associate_plates_to_vehicles(current_vehicles, current_plates)
        else:
            # æ²¡æœ‰è½¦æˆ–è€…æ²¡å¼€è½¦å‹è¯†åˆ«ï¼Œåªæ˜¾ç¤ºè½¦ç‰Œ
            final_detections = current_vehicles # åŒ…å«ç©ºçš„æˆ–è€…ä»…æœ‰è½¦çš„(å¦‚æœæœ‰é€»è¾‘æ¼æ´çš„è¯)
            for p in current_plates:
                final_detections.append({
                    'bbox': p['bbox'],
                    'plate_text': p['text'],
                    'track_id': -1
                })
        
        # --- D. è½¦é€Ÿè¯†åˆ« (å•ç‹¬å¤„ç†ï¼Œè¿½åŠ åˆ°åˆ—è¡¨) ---
        if enable_speed and speed_estimator and speed_estimator.calibrated:
            _, speeds_info = speed_estimator.process_frame(frame, frame_idx)
            for track_id, info in speeds_info.items():
                # æ³¨æ„ï¼šè¿™é‡Œå¯èƒ½ä¼šäº§ç”Ÿé‡å æ¡†ï¼Œå› ä¸ºè½¦é€Ÿæ¨¡å—æœ‰è‡ªå·±çš„æ£€æµ‹å™¨
                # å®Œç¾æ–¹æ¡ˆæ˜¯å°†è½¦é€Ÿæ¨¡å—çš„IDä¸Task1çš„IDå¯¹é½ï¼Œä½†è¿™æ¯”è¾ƒå¤æ‚ã€‚
                # ç°åœ¨çš„å¤„ç†æ˜¯ä½œä¸ºé¢å¤–çš„æ¡†ç»˜åˆ¶ã€‚
                final_detections.append({
                    'track_id': track_id,
                    'bbox': info['bbox'],
                    'speed': info['speed']
                })

        # --- E. ç»Ÿè®¡ä¸ç»˜åˆ¶ ---
        if frame_idx % int(fps) == 0:
            current_total = sum(len(ids) for ids in unique_vehicle_ids.values())
            time_series_data.append({'time': frame_idx / fps, 'count': current_total})

        vis_frame = draw_detection_results(frame, final_detections, 
                                           show_plate=enable_plate, 
                                           show_type=enable_type,
                                           show_speed=enable_speed)
        
        if enable_type:
            y_offset = 30
            for cls_name in sorted(unique_vehicle_ids.keys()):
                count = len(unique_vehicle_ids[cls_name])
                color = vehicle_classifier.get_color(cls_name)
                cv2.putText(vis_frame, f"{cls_name}: {count}", (20, y_offset), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
                y_offset += 30

        writer.write(vis_frame)
        frame_idx += 1
        if progress_callback: progress_callback(frame_idx / total_frames)
            
    cap.release()
    writer.release()
    
    final_counts = {k: len(v) for k, v in unique_vehicle_ids.items()}
    st.session_state.temp_vehicle_counts = final_counts
    st.session_state.temp_time_series = time_series_data
    
    return output_path


def show_calibration_guide():
    """æ˜¾ç¤ºæ ‡å®šæŒ‡å—"""
    with st.expander("ğŸ“– åæ ‡æ ‡å®šå‚è€ƒæŒ‡å—", expanded=True):
        st.markdown("""
        ### æ“ä½œè¯´æ˜
        
        1. åœ¨è§†é¢‘ç”»é¢ä¸­ç‚¹å‡»é€‰æ‹©å…³é”®ç‰¹å¾ç‚¹ï¼ˆå¦‚è½¦é“çº¿ç«¯ç‚¹ã€æ–‘é©¬çº¿è§’ç‚¹ï¼‰ï¼Œå°†å…¶æ ‡è®°ä¸ºåæ ‡åŸç‚¹ $(0, 0)$
        2. æ ¹æ®ä¸‹æ–¹çš„å‚è€ƒæ•°æ®ï¼Œå†é€‰å–åˆ«çš„å‚è€ƒç‚¹ï¼Œä¼°ç®—è¯¥ç‚¹ç›¸å¯¹äºåŸç‚¹çš„è·ç¦»(m)æ ‡æ³¨ $(X, Y)$
        3. è¯·æ‚¨å°½å¯èƒ½å¤šçš„æ ‡æ³¨ç‰¹å¾ç‚¹ï¼ˆå»ºè®®è‡³å°‘6ä¸ªç‚¹ï¼‰
        
        ### å‚è€ƒæ•°æ®
        
        #### è½¦è¡Œé“åˆ†ç•Œçº¿
        - **é«˜é€Ÿå…¬è·¯**ï¼š6-9çº¿ï¼ˆçº¿æ®µé•¿åº¦6mï¼Œé—´éš”9mï¼‰
        - **åŸå¸‚å¿«é€Ÿè·¯**ï¼š4-4çº¿æˆ–4-6çº¿ï¼ˆçº¿æ®µé•¿åº¦4mï¼Œé—´éš”4mæˆ–6mï¼‰
        
        #### è½¦é“å®½åº¦å‚è€ƒè¡¨
        
        | é“è·¯ç±»å‹ | æ ‡å‡†å®½åº¦ (m) | æœ€å°å€¼ (m) |
        |---------|:-----------:|:---------:|
        | é«˜é€Ÿå…¬è·¯ | **3.75** | 3.50 |
        | ä¸€çº§/äºŒçº§å…¬è·¯ | **3.75** | 3.50 |
        | åŸå¸‚å¿«é€Ÿè·¯ | **3.75** | 3.50 |
        | åŸå¸‚æ¬¡å¹²è·¯ | **3.50** | 3.25 |
        | åŸå¸‚æ”¯è·¯ | **3.25** | 2.80 |
        
        #### äººè¡Œæ¨ªé“çº¿
        - æœ€å°å®½åº¦ï¼š3m
        - å¯æŒ‰è¡Œäººæµé‡ä»¥1mä¸ºå•ä½åŠ å®½
        """)


def draw_calibration_points_on_image(image: np.ndarray, points: List[dict], pending_point: tuple = None) -> np.ndarray:
    """
    åœ¨å›¾åƒä¸Šç»˜åˆ¶å·²æ ‡å®šçš„ç‚¹
    
    Args:
        image: è¾“å…¥å›¾åƒ (BGR)
        points: æ ‡å®šç‚¹åˆ—è¡¨
        pending_point: å¾…ç¡®è®¤çš„ç‚¹ (px, py)ï¼Œç”¨é»„è‰²æ˜¾ç¤º
    
    Returns:
        ç»˜åˆ¶åçš„å›¾åƒ
    """
    vis = image.copy()
    
    # ç»˜åˆ¶å·²ç¡®è®¤çš„ç‚¹ï¼ˆç»¿è‰²ï¼‰
    for i, pt in enumerate(points):
        px, py = int(pt['px']), int(pt['py'])
        # ç»˜åˆ¶åœ†ç‚¹
        cv2.circle(vis, (px, py), 8, (0, 255, 0), -1)
        cv2.circle(vis, (px, py), 10, (255, 255, 255), 2)
        # ç»˜åˆ¶æ ‡å·
        label = f"P{i+1}"
        cv2.putText(vis, label, (px + 12, py + 5), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        cv2.putText(vis, label, (px + 12, py + 5), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)
    
    # ç»˜åˆ¶å¾…ç¡®è®¤çš„ç‚¹ï¼ˆé»„è‰²ï¼‰
    if pending_point is not None:
        px, py = int(pending_point[0]), int(pending_point[1])
        cv2.circle(vis, (px, py), 10, (0, 255, 255), -1)
        cv2.circle(vis, (px, py), 12, (255, 255, 255), 2)
        label = f"P{len(points)+1}?"
        cv2.putText(vis, label, (px + 14, py + 5), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        cv2.putText(vis, label, (px + 14, py + 5), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 1)
    
    return vis


def calibration_interface(first_frame: np.ndarray, speed_estimator) -> bool:
    """
    æ ‡å®šç•Œé¢ - æ”¯æŒäº¤äº’å¼ç‚¹å‡»é€‰ç‚¹
    
    Returns:
        bool: æ ‡å®šæ˜¯å¦å®Œæˆ
    """
    st.subheader("ğŸ¯ è·ç¦»æ ‡å®š")
    
    # æ˜¾ç¤ºæŒ‡å—
    show_calibration_guide()
    
    # åˆå§‹åŒ– session_state
    if 'calib_points' not in st.session_state:
        st.session_state.calib_points = []  # å·²ç¡®è®¤çš„æ ‡å®šç‚¹åˆ—è¡¨
    if 'pending_click' not in st.session_state:
        st.session_state.pending_click = None  # å¾…ç¡®è®¤çš„ç‚¹å‡»åæ ‡ (px, py)
    if 'last_click_coords' not in st.session_state:
        st.session_state.last_click_coords = None
    
    # è·å–åŸå§‹å›¾åƒå°ºå¯¸
    orig_height, orig_width = first_frame.shape[:2]
    
    # ===== äº¤äº’å¼ç‚¹å‡»åŒºåŸŸ =====
    st.markdown("### ğŸ“ ç‚¹å‡»å›¾ç‰‡æ·»åŠ æ ‡å®šç‚¹")
    st.info("ğŸ’¡ **æ“ä½œæ­¥éª¤**ï¼šç‚¹å‡»å›¾ç‰‡é€‰æ‹©ä¸€ä¸ªç‰¹å¾ç‚¹ â†’ è¾“å…¥è¯¥ç‚¹çš„ç›¸å¯¹åæ ‡(ç±³) â†’ ç‚¹å‡»ã€Œç¡®è®¤æ·»åŠ ã€")
    
    # åœ¨å›¾åƒä¸Šç»˜åˆ¶å·²æ ‡å®šçš„ç‚¹å’Œå¾…ç¡®è®¤çš„ç‚¹
    display_image = draw_calibration_points_on_image(
        first_frame, 
        st.session_state.calib_points,
        st.session_state.pending_click
    )
    
    # è½¬æ¢ä¸º RGB ç”¨äºæ˜¾ç¤º
    display_image_rgb = cv2.cvtColor(display_image, cv2.COLOR_BGR2RGB)
    pil_image = Image.fromarray(display_image_rgb)
    
    # è®¡ç®—æ˜¾ç¤ºå°ºå¯¸ï¼ˆä¿æŒå®½é«˜æ¯”ï¼Œæœ€å¤§å®½åº¦800ï¼‰
    max_display_width = 800
    scale = min(max_display_width / orig_width, 1.0)
    display_width = int(orig_width * scale)
    display_height = int(orig_height * scale)
    
    # ä½¿ç”¨ streamlit_image_coordinates è·å–ç‚¹å‡»åæ ‡
    clicked_coords = streamlit_image_coordinates(
        pil_image,
        width=display_width,
        height=display_height,
        key="calibration_image"
    )
    
    # å¤„ç†ç‚¹å‡»äº‹ä»¶
    if clicked_coords is not None:
        click_x = int(clicked_coords['x'] / scale)
        click_y = int(clicked_coords['y'] / scale)
        current_click = (click_x, click_y)
        
        # åªæœ‰å½“è¿™æ˜¯ä¸€ä¸ªæ–°çš„ç‚¹å‡»æ—¶æ‰æ›´æ–°å¾…ç¡®è®¤ç‚¹
        if st.session_state.last_click_coords != current_click:
            st.session_state.last_click_coords = current_click
            st.session_state.pending_click = current_click
            st.rerun()
    
    st.caption(f"å›¾åƒå°ºå¯¸: {orig_width} x {orig_height} åƒç´  | å·²æ ‡å®š {len(st.session_state.calib_points)} ä¸ªç‚¹")
    
    # ===== æ·»åŠ æ–°ç‚¹çš„è¾“å…¥åŒºåŸŸ =====
    if st.session_state.pending_click is not None:
        st.markdown("---")
        st.markdown(f"### â• æ·»åŠ æ–°æ ‡å®šç‚¹ P{len(st.session_state.calib_points) + 1}")
        
        px, py = st.session_state.pending_click
        
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**ğŸ“ åƒç´ åæ ‡ï¼ˆå·²ä»å›¾ç‰‡è·å–ï¼‰**")
            disp_col1, disp_col2 = st.columns(2)
            disp_col1.metric("åƒç´  X", px)
            disp_col2.metric("åƒç´  Y", py)
        
        with col2:
            st.markdown("**ğŸŒ ç›¸å¯¹åæ ‡ï¼ˆè¯·è¾“å…¥ï¼Œå•ä½ï¼šç±³ï¼‰**")
            input_col1, input_col2 = st.columns(2)
            world_x = input_col1.number_input("X (m)", value=0.0, format="%.1f", key="new_wx")
            world_y = input_col2.number_input("Y (m)", value=0.0, format="%.1f", key="new_wy")
        
        # æ·»åŠ å’Œå–æ¶ˆæŒ‰é’®
        btn_col1, btn_col2 = st.columns(2)
        
        if btn_col1.button("âœ… ç¡®è®¤æ·»åŠ ", type="primary", key="add_point_btn"):
            # æ·»åŠ æ–°ç‚¹åˆ°åˆ—è¡¨
            new_point = {
                'px': px,
                'py': py,
                'wx': world_x,
                'wy': world_y
            }
            st.session_state.calib_points.append(new_point)
            st.session_state.pending_click = None
            st.toast(f"âœ… P{len(st.session_state.calib_points)} å·²æ·»åŠ ")
            st.rerun()
        
        if btn_col2.button("âŒ å–æ¶ˆ", key="cancel_point_btn"):
            st.session_state.pending_click = None
            st.rerun()
    else:
        st.info("ğŸ‘† è¯·ç‚¹å‡»ä¸Šæ–¹å›¾ç‰‡é€‰æ‹©ä¸€ä¸ªæ ‡å®šç‚¹")
    
    # ===== å·²ç¡®è®¤çš„æ ‡å®šç‚¹åˆ—è¡¨ =====
    st.markdown("---")
    st.markdown("### ğŸ“‹ å·²æ·»åŠ çš„æ ‡å®šç‚¹")
    
    if len(st.session_state.calib_points) == 0:
        st.warning("æš‚æ— æ ‡å®šç‚¹ï¼Œè¯·ç‚¹å‡»å›¾ç‰‡æ·»åŠ ")
    else:
        # è¡¨å¤´
        cols_header = st.columns([1, 2, 2, 2, 2, 1])
        cols_header[0].write("**ç‚¹**")
        cols_header[1].write("**åƒç´ X**")
        cols_header[2].write("**åƒç´ Y**")
        cols_header[3].write("**ç›¸å¯¹X(m)**")
        cols_header[4].write("**ç›¸å¯¹Y(m)**")
        cols_header[5].write("**æ“ä½œ**")
        
        # æ˜¾ç¤ºæ¯ä¸ªæ ‡å®šç‚¹
        points_to_remove = []
        for i, pt in enumerate(st.session_state.calib_points):
            cols = st.columns([1, 2, 2, 2, 2, 1])
            cols[0].write(f"**P{i+1}**")
            cols[1].write(f"{int(pt['px'])}")
            cols[2].write(f"{int(pt['py'])}")
            cols[3].write(f"{pt['wx']:.1f}")
            cols[4].write(f"{pt['wy']:.1f}")
            
            if cols[5].button("ğŸ—‘ï¸", key=f"del_btn_{i}", help="åˆ é™¤æ­¤ç‚¹"):
                points_to_remove.append(i)
        
        # åˆ é™¤æ ‡è®°çš„ç‚¹
        if points_to_remove:
            for idx in sorted(points_to_remove, reverse=True):
                st.session_state.calib_points.pop(idx)
            st.rerun()
    
    # ===== æœ€ç»ˆæ“ä½œæŒ‰é’® =====
    st.markdown("---")
    
    col_btn1, col_btn2 = st.columns(2)
    
    # æ¸…é™¤æ‰€æœ‰ç‚¹æŒ‰é’®
    if col_btn1.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰æ ‡å®šç‚¹", key="clear_all_points"):
        st.session_state.calib_points = []
        st.session_state.pending_click = None
        st.session_state.last_click_coords = None
        st.session_state.calibration_step = 'adding_points'
        st.rerun()
    
    # ç¡®è®¤æ ‡å®šæŒ‰é’®
    num_valid_points = len(st.session_state.calib_points)
    can_calibrate = num_valid_points >= 4
    
    if col_btn2.button("âœ… å®Œæˆæ ‡å®š", type="primary", key="confirm_calib", disabled=not can_calibrate):
        # æ”¶é›†æ ‡å®šç‚¹
        pixel_points = [(pt['px'], pt['py']) for pt in st.session_state.calib_points]
        world_points = [(pt['wx'], pt['wy']) for pt in st.session_state.calib_points]
        
        # æ‰§è¡Œæ ‡å®š
        success = speed_estimator.calibrate_from_points(pixel_points, world_points)
        
        if success:
            st.session_state.calibration_step = 'ask_validation'
            st.session_state.temp_calibration_error = speed_estimator.calibration_error
            st.session_state.temp_num_points = num_valid_points
            st.rerun()
        else:
            st.error("æ ‡å®šå¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å…¥çš„ç‚¹æ˜¯å¦æ­£ç¡®")
            return False
    
    if not can_calibrate:
        st.warning(f"âš ï¸ éœ€è¦è‡³å°‘ 4 ä¸ªæ ‡å®šç‚¹æ‰èƒ½å®Œæˆæ ‡å®šï¼ˆå½“å‰: {num_valid_points} ä¸ªï¼‰")
            
    return False


def validation_interface(first_frame: np.ndarray, speed_estimator) -> bool:
    """
    éªŒè¯æ ‡å®šç•Œé¢ - åœ¨æ ‡å®šå®Œæˆåæ˜¾ç¤º
    
    Returns:
        bool: æ˜¯å¦å®Œæˆï¼ˆè·³è¿‡æˆ–éªŒè¯å®Œæˆï¼‰
    """
    st.subheader("ğŸ” éªŒè¯æ ‡å®š")
    
    # æ˜¾ç¤ºæ ‡å®šæˆåŠŸä¿¡æ¯
    st.success(f"âœ… æ ‡å®šæˆåŠŸï¼ä½¿ç”¨äº† {st.session_state.temp_num_points} ä¸ªç‚¹ï¼Œæ ‡å®šè¯¯å·®: {st.session_state.temp_calibration_error:.2f} åƒç´ ")
    
    st.markdown("---")
    st.markdown("### æ˜¯å¦éœ€è¦éªŒè¯æ ‡å®šç²¾åº¦ï¼Ÿ")
    st.info("ğŸ’¡ æ‚¨å¯ä»¥é€‰æ‹©ä¸€æ¡è½¦é“çš„å·¦å³è¾¹ç¼˜ä¸¤ç‚¹ï¼Œç³»ç»Ÿä¼šè®¡ç®—è½¦é“å®½åº¦æ¥éªŒè¯æ ‡å®šç²¾åº¦ã€‚æ ‡å‡†è½¦é“å®½åº¦çº¦ä¸º **3.75ç±³**ã€‚")
    
    # åˆå§‹åŒ–éªŒè¯çŠ¶æ€
    if 'validation_step' not in st.session_state:
        st.session_state.validation_step = 'ask'  # 'ask', 'selecting', 'done'
    if 'val_left_point' not in st.session_state:
        st.session_state.val_left_point = None
    if 'val_right_point' not in st.session_state:
        st.session_state.val_right_point = None
    if 'val_selecting' not in st.session_state:
        st.session_state.val_selecting = None  # 'left' or 'right'
    
    # è¯¢é—®æ˜¯å¦éªŒè¯
    if st.session_state.validation_step == 'ask':
        col1, col2 = st.columns(2)
        
        if col1.button("âœ… è¿›è¡ŒéªŒè¯", type="primary", key="do_validation"):
            st.session_state.validation_step = 'selecting'
            st.session_state.val_selecting = 'left'
            st.rerun()
        
        if col2.button("â­ï¸ è·³è¿‡éªŒè¯", key="skip_validation"):
            st.session_state.calibration_done = True
            st.session_state.calibration_step = 'done'
            return True
    
    # é€‰æ‹©éªŒè¯ç‚¹
    elif st.session_state.validation_step == 'selecting':
        # è·å–åŸå§‹å›¾åƒå°ºå¯¸
        orig_height, orig_width = first_frame.shape[:2]
        
        # åœ¨å›¾åƒä¸Šç»˜åˆ¶éªŒè¯ç‚¹
        display_image = first_frame.copy()
        
        # ç»˜åˆ¶å·¦è¾¹ç¼˜ç‚¹ï¼ˆè“è‰²ï¼‰
        if st.session_state.val_left_point:
            px, py = st.session_state.val_left_point
            cv2.circle(display_image, (px, py), 10, (255, 100, 0), -1)
            cv2.circle(display_image, (px, py), 12, (255, 255, 255), 2)
            cv2.putText(display_image, "L", (px + 14, py + 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # ç»˜åˆ¶å³è¾¹ç¼˜ç‚¹ï¼ˆçº¢è‰²ï¼‰
        if st.session_state.val_right_point:
            px, py = st.session_state.val_right_point
            cv2.circle(display_image, (px, py), 10, (0, 100, 255), -1)
            cv2.circle(display_image, (px, py), 12, (255, 255, 255), 2)
            cv2.putText(display_image, "R", (px + 14, py + 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # å¦‚æœä¸¤ç‚¹éƒ½æœ‰ï¼Œç»˜åˆ¶è¿çº¿
        if st.session_state.val_left_point and st.session_state.val_right_point:
            cv2.line(display_image, st.session_state.val_left_point, 
                    st.session_state.val_right_point, (0, 255, 255), 2)
        
        # è½¬æ¢æ˜¾ç¤º
        display_image_rgb = cv2.cvtColor(display_image, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(display_image_rgb)
        
        max_display_width = 800
        scale = min(max_display_width / orig_width, 1.0)
        display_width = int(orig_width * scale)
        display_height = int(orig_height * scale)
        
        # æ˜¾ç¤ºå½“å‰é€‰æ‹©çŠ¶æ€
        if st.session_state.val_selecting == 'left':
            st.markdown("### ğŸ“ è¯·ç‚¹å‡»å›¾ç‰‡é€‰æ‹© **è½¦é“å·¦è¾¹ç¼˜** ç‚¹")
            st.info("ğŸ”µ ç‚¹å‡»å›¾ç‰‡é€‰æ‹©è½¦é“å·¦è¾¹ç¼˜çš„ä¸€ä¸ªç‚¹")
        elif st.session_state.val_selecting == 'right':
            st.markdown("### ğŸ“ è¯·ç‚¹å‡»å›¾ç‰‡é€‰æ‹© **è½¦é“å³è¾¹ç¼˜** ç‚¹")
            st.info("ğŸ”´ ç‚¹å‡»å›¾ç‰‡é€‰æ‹©è½¦é“å³è¾¹ç¼˜çš„ä¸€ä¸ªç‚¹")
        else:
            st.markdown("### ğŸ“ éªŒè¯ç‚¹é€‰æ‹©å®Œæˆ")
        
        # ç‚¹å‡»å›¾ç‰‡
        clicked_coords = streamlit_image_coordinates(
            pil_image,
            width=display_width,
            height=display_height,
            key="validation_image"
        )
        
        # å¤„ç†ç‚¹å‡»
        if clicked_coords is not None:
            click_x = int(clicked_coords['x'] / scale)
            click_y = int(clicked_coords['y'] / scale)
            current_click = (click_x, click_y)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°ç‚¹å‡»
            last_val_click = st.session_state.get('last_val_click', None)
            if last_val_click != current_click:
                st.session_state.last_val_click = current_click
                
                if st.session_state.val_selecting == 'left':
                    st.session_state.val_left_point = current_click
                    st.session_state.val_selecting = 'right'
                    st.toast(f"âœ… å·¦è¾¹ç¼˜ç‚¹å·²é€‰æ‹©: ({click_x}, {click_y})")
                    st.rerun()
                elif st.session_state.val_selecting == 'right':
                    st.session_state.val_right_point = current_click
                    st.session_state.val_selecting = None
                    st.toast(f"âœ… å³è¾¹ç¼˜ç‚¹å·²é€‰æ‹©: ({click_x}, {click_y})")
                    st.rerun()
        
        # æ˜¾ç¤ºå½“å‰é€‰æ‹©çš„ç‚¹
        st.markdown("---")
        col1, col2 = st.columns(2)
        with col1:
            if st.session_state.val_left_point:
                st.metric("ğŸ”µ å·¦è¾¹ç¼˜ç‚¹", f"({st.session_state.val_left_point[0]}, {st.session_state.val_left_point[1]})")
            else:
                st.metric("ğŸ”µ å·¦è¾¹ç¼˜ç‚¹", "æœªé€‰æ‹©")
        with col2:
            if st.session_state.val_right_point:
                st.metric("ğŸ”´ å³è¾¹ç¼˜ç‚¹", f"({st.session_state.val_right_point[0]}, {st.session_state.val_right_point[1]})")
            else:
                st.metric("ğŸ”´ å³è¾¹ç¼˜ç‚¹", "æœªé€‰æ‹©")
        
        # æ“ä½œæŒ‰é’®
        st.markdown("---")
        btn_col1, btn_col2, btn_col3 = st.columns(3)
        
        # é‡æ–°é€‰æ‹©å·¦è¾¹ç¼˜
        if btn_col1.button("ğŸ”„ é‡é€‰å·¦è¾¹ç¼˜", key="reselect_left"):
            st.session_state.val_selecting = 'left'
            st.session_state.last_val_click = None
            st.rerun()
        
        # é‡æ–°é€‰æ‹©å³è¾¹ç¼˜
        if btn_col2.button("ğŸ”„ é‡é€‰å³è¾¹ç¼˜", key="reselect_right"):
            st.session_state.val_selecting = 'right'
            st.session_state.last_val_click = None
            st.rerun()
        
        # æ‰§è¡ŒéªŒè¯
        can_validate = st.session_state.val_left_point is not None and st.session_state.val_right_point is not None
        
        if btn_col3.button("âœ… æ‰§è¡ŒéªŒè¯", type="primary", key="run_validation", disabled=not can_validate):
            # è®¡ç®—è½¦é“å®½åº¦
            width, status = speed_estimator.validate_lane_width(
                st.session_state.val_left_point,
                st.session_state.val_right_point
            )
            st.session_state.validation_result = (width, status)
            st.session_state.validation_step = 'done'
            st.rerun()
        
        # è·³è¿‡éªŒè¯
        st.markdown("---")
        if st.button("â­ï¸ è·³è¿‡éªŒè¯ï¼Œç›´æ¥å¼€å§‹æ£€æµ‹", key="skip_validation_2"):
            st.session_state.calibration_done = True
            st.session_state.calibration_step = 'done'
            return True
    
    # æ˜¾ç¤ºéªŒè¯ç»“æœ
    elif st.session_state.validation_step == 'done':
        if 'validation_result' in st.session_state:
            width, status = st.session_state.validation_result
            
            st.markdown("### ğŸ“Š éªŒè¯ç»“æœ")
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("è®¡ç®—çš„è½¦é“å®½åº¦", f"{width:.2f} ç±³")
            with col2:
                st.metric("æ ‡å‡†è½¦é“å®½åº¦", "3.75 ç±³")
            
            st.markdown(f"**è¯„ä¼°ç»“æœ**: {status}")
        
        st.markdown("---")
        if st.button("âœ… å®Œæˆï¼Œå¼€å§‹æ£€æµ‹", type="primary", key="finish_validation"):
            st.session_state.calibration_done = True
            st.session_state.calibration_step = 'done'
            return True
    
    return False


def main():
    """ä¸»å‡½æ•°"""
    # æ ‡é¢˜
    st.title("ğŸš— æ™ºèƒ½äº¤é€šè¯†åˆ«ç³»ç»Ÿ")
    st.markdown("---")
    
    # åˆå§‹åŒ– session_state
    if 'uploaded_file_name' not in st.session_state:
        st.session_state.uploaded_file_name = None
    if 'calibration_done' not in st.session_state:
        st.session_state.calibration_done = False
    if 'processing_done' not in st.session_state:
        st.session_state.processing_done = False
        
    # ==================== ä¾§è¾¹æ  ====================
    with st.sidebar:
        st.header("æ§åˆ¶é¢æ¿")
        
        # æ–‡ä»¶ä¸Šä¼ 
        st.subheader("ğŸ“ æ–‡ä»¶ä¸Šä¼ ")
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ å›¾ç‰‡æˆ–è§†é¢‘",
            type=IMAGE_EXTENSIONS + VIDEO_EXTENSIONS,
            help="æ”¯æŒæ ¼å¼ï¼šJPG, PNG, MP4, AVI, MOV",
        )
        
        # æ£€æµ‹æ–‡ä»¶å˜åŒ–ï¼Œé‡ç½®çŠ¶æ€
        if uploaded_file:
            if st.session_state.uploaded_file_name != uploaded_file.name:
                st.session_state.uploaded_file_name = uploaded_file.name
                st.session_state.calibration_done = False
                st.session_state.processing_done = False
                st.session_state.calibration_step = 'adding_points'
                # é‡ç½®æ ‡å®šç›¸å…³çŠ¶æ€
                if 'calib_points' in st.session_state:
                    st.session_state.calib_points = []
                if 'pending_click' in st.session_state:
                    st.session_state.pending_click = None
                if 'validation_step' in st.session_state:
                    st.session_state.validation_step = 'ask'
                if 'val_left_point' in st.session_state:
                    st.session_state.val_left_point = None
                if 'val_right_point' in st.session_state:
                    st.session_state.val_right_point = None
                
        # åŠŸèƒ½é€‰æ‹©
        st.subheader("ğŸ”§ åŠŸèƒ½é€‰æ‹©")
        
        # 1. è½¦å‹è¯†åˆ« (é»˜è®¤ä¸å‹¾é€‰)
        enable_type = st.checkbox("è½¦å‹è¯†åˆ«", value=False, key="enable_type")
        
        # 2. è½¦ç‰Œè¯†åˆ« (é»˜è®¤ä¸å‹¾é€‰)
        enable_plate = st.checkbox("è½¦ç‰Œè¯†åˆ«", value=False, key="enable_plate")
        
        # 3. è½¦é€Ÿè¯†åˆ« (é»˜è®¤ä¸å‹¾é€‰ï¼Œå§‹ç»ˆæ˜¾ç¤ºï¼Œä½†åœ¨éè§†é¢‘æ¨¡å¼ä¸‹ç¦ç”¨)
        # åˆ¤æ–­å½“å‰æ–‡ä»¶çŠ¶æ€
        is_video = False
        if uploaded_file:
            is_video = is_video_file(uploaded_file.name)
            
        # ç¦ç”¨æ¡ä»¶ï¼šå·²ä¸Šä¼ æ–‡ä»¶ ä¸” ä¸æ˜¯è§†é¢‘
        speed_disabled = (uploaded_file is not None) and (not is_video)
        
        # æ¸²æŸ“å¤é€‰æ¡† (åˆ©ç”¨ disabled å‚æ•°æ§åˆ¶æ˜¯å¦å¯é€‰)
        enable_speed = st.checkbox("è½¦é€Ÿè¯†åˆ«", value=False, key="enable_speed", disabled=speed_disabled)
        
        # é¢å¤–çš„ UI æç¤ºå’Œé€»è¾‘å®‰å…¨é”
        if speed_disabled:
            st.caption("ğŸ’¡ å›¾ç‰‡æ¨¡å¼ä¸æ”¯æŒè½¦é€Ÿæ£€æµ‹")
            enable_speed = False  # å¼ºåˆ¶è®¾ä¸º Falseï¼Œé˜²æ­¢é€»è¾‘é”™è¯¯
                
        # å¼€å§‹æ£€æµ‹æŒ‰é’®
        st.subheader("ğŸš€ æ“ä½œ")
        
        # æŒ‰é’®å¯ç”¨æ¡ä»¶
        can_start = uploaded_file is not None
        if enable_speed and not st.session_state.calibration_done:
            can_start = False
            st.warning("âš ï¸ è¯·å…ˆå®Œæˆè½¦é€Ÿæ ‡å®š")
            
        start_button = st.button(
            "å¼€å§‹æ£€æµ‹", 
            type="primary", 
            disabled=not can_start,
            key="start_detection"
        )
        
    # ==================== ä¸»å±•ç¤ºåŒº ====================
    
    # é»˜è®¤çŠ¶æ€
    if not uploaded_file:
        st.info("ğŸ‘‹ æ¬¢è¿ä½¿ç”¨æ™ºèƒ½äº¤é€šè¯†åˆ«ç³»ç»Ÿï¼è¯·åœ¨å·¦ä¾§ä¸Šä¼ å›¾ç‰‡æˆ–è§†é¢‘å¼€å§‹ä½¿ç”¨ã€‚")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.markdown("### ğŸš™ è½¦å‹è¯†åˆ«")
            st.write("è¯†åˆ«è½¦è¾†ç±»å‹ï¼ˆè½¿è½¦ã€SUVã€è´§è½¦ç­‰ï¼‰")
        with col2:
            st.markdown("### ğŸ”¢ è½¦ç‰Œè¯†åˆ«")
            st.write("æ£€æµ‹å¹¶è¯†åˆ«è½¦ç‰Œå·ç ")
        with col3:
            st.markdown("### âš¡ è½¦é€Ÿè¯†åˆ«")
            st.write("ä¼°ç®—è§†é¢‘ä¸­è½¦è¾†çš„è¡Œé©¶é€Ÿåº¦ï¼ˆä»…è§†é¢‘ï¼‰")
            
        return
        
    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
    temp_input_path = TEMP_DIR / uploaded_file.name
    with open(temp_input_path, 'wb') as f:
        f.write(uploaded_file.read())
        
    # åˆ¤æ–­æ–‡ä»¶ç±»å‹
    is_image = is_image_file(uploaded_file.name)
    is_video = is_video_file(uploaded_file.name)
    
    # ========== å›¾ç‰‡å¤„ç† ==========
    if is_image:
        if start_button:
            with st.spinner("æ­£åœ¨å¤„ç†..."):
                # è¯»å–å›¾ç‰‡
                image = cv2.imread(str(temp_input_path))
                
                if image is None:
                    st.error("æ— æ³•è¯»å–å›¾ç‰‡æ–‡ä»¶")
                    return
                    
                # å¤„ç†å›¾ç‰‡
                vis_image, df = process_image(image, enable_plate, enable_type)
                
                st.session_state.processing_done = True
                st.session_state.result_image = vis_image
                st.session_state.result_df = df
                
        # æ˜¾ç¤ºç»“æœ
        if st.session_state.processing_done and hasattr(st.session_state, 'result_image'):
            st.subheader("ğŸ“· æ£€æµ‹ç»“æœ")
            
            # æ˜¾ç¤ºå¤„ç†åçš„å›¾ç‰‡
            st.image(cv2.cvtColor(st.session_state.result_image, cv2.COLOR_BGR2RGB), 
                     caption="å¤„ç†ç»“æœ", use_container_width=True)
            
            # æ–°å¢ï¼šæ˜¾ç¤ºç»Ÿè®¡å›¾è¡¨
            if hasattr(st.session_state, 'temp_vehicle_counts'):
                st.subheader("ğŸ“Š æ•°æ®ç»Ÿè®¡")
                draw_statistics_charts(st.session_state.temp_vehicle_counts, None)
            
            # æ˜¾ç¤ºç»Ÿè®¡è¡¨æ ¼
            if not st.session_state.result_df.empty:
                st.subheader("ğŸ“Š æ£€æµ‹ç»Ÿè®¡")
                st.dataframe(st.session_state.result_df, use_container_width=True)
            else:
                st.info("æœªæ£€æµ‹åˆ°è½¦è¾†/è½¦ç‰Œ")
                
            # ä¸‹è½½æŒ‰é’®
            _, ext = os.path.splitext(uploaded_file.name)
            output_filename = f"result_{int(time.time())}{ext}"
            
            # ç¼–ç å›¾ç‰‡
            _, buffer = cv2.imencode(ext, st.session_state.result_image)
            
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½å¤„ç†åçš„å›¾ç‰‡",
                data=buffer.tobytes(),
                file_name=output_filename,
                mime=f"image/{ext[1:]}"
            )
        else:
            # æ˜¾ç¤ºåŸå›¾é¢„è§ˆ
            st.subheader("ğŸ“· å›¾ç‰‡é¢„è§ˆ")
            image = cv2.imread(str(temp_input_path))
            if image is not None:
                st.image(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), caption="åŸå›¾", use_container_width=True)
                
    # ========== è§†é¢‘å¤„ç† ==========
    elif is_video:
        # åˆå§‹åŒ–æ ‡å®šæ­¥éª¤çŠ¶æ€
        if 'calibration_step' not in st.session_state:
            st.session_state.calibration_step = 'adding_points'  # 'adding_points', 'ask_validation', 'done'
        
        # è½¦é€Ÿæ ‡å®šç•Œé¢
        if enable_speed and not st.session_state.calibration_done:
            speed_estimator = load_speed_estimator()
            
            if speed_estimator:
                first_frame = speed_estimator.get_first_frame(str(temp_input_path))
                
                if first_frame is not None:
                    # æ ¹æ®æ ‡å®šæ­¥éª¤æ˜¾ç¤ºä¸åŒç•Œé¢
                    if st.session_state.calibration_step == 'adding_points':
                        calibration_interface(first_frame, speed_estimator)
                    elif st.session_state.calibration_step == 'ask_validation':
                        # éœ€è¦é‡æ–°æ‰§è¡Œæ ‡å®šä»¥è·å¾— speed_estimator çš„çŠ¶æ€
                        pixel_points = [(pt['px'], pt['py']) for pt in st.session_state.calib_points]
                        world_points = [(pt['wx'], pt['wy']) for pt in st.session_state.calib_points]
                        speed_estimator.calibrate_from_points(pixel_points, world_points)
                        validation_interface(first_frame, speed_estimator)
                else:
                    st.error("æ— æ³•è¯»å–è§†é¢‘ç¬¬ä¸€å¸§")
            return
            
        # å¤„ç†è§†é¢‘
        if start_button:
            speed_estimator = load_speed_estimator() if enable_speed else None
            
            # å¦‚æœå·²æ ‡å®šï¼Œéœ€è¦é‡æ–°åŠ è½½å¹¶è®¾ç½®æ ‡å®šå‚æ•°
            if enable_speed and st.session_state.calibration_done:
                # é‡æ–°æ ‡å®š
                pixel_points = [(pt['px'], pt['py']) for pt in st.session_state.calib_points]
                world_points = [(pt['wx'], pt['wy']) for pt in st.session_state.calib_points]
                speed_estimator.calibrate_from_points(pixel_points, world_points)
                
            st.subheader("â³ æ­£åœ¨å¤„ç†è§†é¢‘...")
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            def update_progress(progress):
                progress_bar.progress(progress)
                status_text.text(f"å¤„ç†è¿›åº¦: {progress*100:.1f}%")
                
            try:
                output_path = process_video(
                    str(temp_input_path),
                    enable_plate=enable_plate,
                    enable_type=enable_type,
                    enable_speed=enable_speed,
                    speed_estimator=speed_estimator,
                    progress_callback=update_progress
                )
                
                st.session_state.processing_done = True
                st.session_state.result_video_path = output_path
                
                progress_bar.progress(1.0)
                status_text.text("å¤„ç†å®Œæˆï¼")
                
            except Exception as e:
                st.error(f"è§†é¢‘å¤„ç†å¤±è´¥: {e}")
                return
                
        # æ˜¾ç¤ºç»“æœ
        if st.session_state.processing_done and hasattr(st.session_state, 'result_video_path'):
            st.subheader("ğŸ¬ å¤„ç†ç»“æœ")
            
            # æ˜¾ç¤ºè§†é¢‘
            with open(st.session_state.result_video_path, 'rb') as f:
                video_bytes = f.read()
                st.video(video_bytes)

            # æ–°å¢ï¼šæ˜¾ç¤ºç»Ÿè®¡å›¾è¡¨ (é¥¼å›¾ + æŠ˜çº¿å›¾)
            if hasattr(st.session_state, 'temp_vehicle_counts') and hasattr(st.session_state, 'temp_time_series'):
                st.subheader("ğŸ“Š äº¤é€šæ•°æ®åˆ†æ")
                draw_statistics_charts(st.session_state.temp_vehicle_counts, 
                                    st.session_state.temp_time_series)
                
            # ä¸‹è½½æŒ‰é’®
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½å¤„ç†åçš„è§†é¢‘",
                data=video_bytes,
                file_name=f"result_{int(time.time())}.mp4",
                mime="video/mp4"
            )
        else:
            # æ˜¾ç¤ºåŸè§†é¢‘é¢„è§ˆ
            st.subheader("ğŸ¬ è§†é¢‘é¢„è§ˆ")
            st.video(str(temp_input_path))


if __name__ == "__main__":
    main()
