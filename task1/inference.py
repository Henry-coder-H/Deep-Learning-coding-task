import torch
from ultralytics import YOLO
import cv2
import os
import argparse
import time
import warnings
from collections import defaultdict
import matplotlib.pyplot as plt  # æ–°å¢ï¼šç”¨äºç»˜å›¾
import pandas as pd             # æ–°å¢ï¼šç”¨äºæ•°æ®ç®¡ç†

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore")

# ================= ğŸ”§ å…¨å±€é…ç½® =================
COLORS = {
    'Bus': (0, 128, 255), 'Microbus': (0, 255, 255), 'Minivan': (255, 0, 255),
    'Sedan': (0, 255, 0), 'SUV': (255, 0, 0), 'Truck': (0, 0, 255)
}
DEFAULT_COLOR = (255, 255, 255)

class InferenceEngine:
    def __init__(self, model_path):
        print(f"ğŸ‘‰ [åˆå§‹åŒ–] åŠ è½½ YOLO æ¨¡å‹: {model_path}")
        self.model = YOLO(model_path)
        self.class_names = self.model.names
        
        # --- ğŸ“Š ç»Ÿè®¡ç›¸å…³æ•°æ®ç»“æ„ ---
        self.vehicle_counts = defaultdict(set) # è®°å½•ä¸é‡å¤çš„ ID
        self.time_series_data = []             # è®°å½• (æ—¶é—´ç‚¹, å®æ—¶è½¦è¾†æ€»æ•°)

    def run(self, input_path, output_path, report_path):
        ext = os.path.splitext(input_path)[1].lower()
        if ext in ['.jpg', '.jpeg', '.png']:
            self._process_image(input_path, output_path, report_path)
        elif ext in ['.mp4', '.avi', '.mov']:
            self._process_video(input_path, output_path, report_path)

    def _process_image(self, img_path, save_path, report_path=None):
        """
        å¤„ç†å•å¼ å›¾ç‰‡ï¼šæ¨ç† -> ç»Ÿè®¡ -> ç»˜å›¾ -> ä¿å­˜ -> ç”ŸæˆæŠ¥å‘Š
        """
        print(f"ğŸ–¼ï¸ [å›¾ç‰‡] å¼€å§‹å¤„ç†: {img_path}")
        img = cv2.imread(img_path)
        if img is None:
            print(f"âŒ é”™è¯¯: æ— æ³•è¯»å–å›¾ç‰‡ {img_path}")
            return

        # 1. æ¨ç† (å›¾ç‰‡æ— éœ€è·Ÿè¸ªæ¨¡å¼ï¼Œä½¿ç”¨ predict å³å¯)
        results = self.model.predict(img, conf=0.25, verbose=False)[0]

        # 2. ç»Ÿè®¡é€»è¾‘é€‚é…
        # å›¾ç‰‡æ¨¡å¼ä¸‹æ²¡æœ‰ Track IDï¼Œä¸ºäº†é€‚é… self.vehicle_counts çš„ set ç»“æ„ï¼Œ
        # æˆ‘ä»¬ä½¿ç”¨å½“å‰å¸§çš„æ£€æµ‹æ¡†ç´¢å¼•(index)ä½œä¸º"ä¼ªID"è¿›è¡Œè®¡æ•°ã€‚
        if results.boxes:
            cls_ids = results.boxes.cls.int().cpu().numpy()
            for i, c_id in enumerate(cls_ids):
                class_name = self.class_names[c_id]
                # ä½¿ç”¨ i ä½œä¸ºä¸´æ—¶å”¯ä¸€æ ‡è¯†ï¼Œç¡®ä¿ len(set) ç»Ÿè®¡æ­£ç¡®
                self.vehicle_counts[class_name].add(i)

        # 3. ç»˜å›¾ (å¤ç”¨ç°æœ‰æ–¹æ³•)
        self._draw_results(img, results, is_video=False)
        self._draw_statistics_panel(img)

        # 4. æ™ºèƒ½ä¿®æ­£ä¿å­˜è·¯å¾„åç¼€
        # å¦‚æœä¸»ç¨‹åºä¼ å…¥çš„æ˜¯ .mp4 åç¼€ï¼ˆé’ˆå¯¹è§†é¢‘çš„é»˜è®¤è®¾ç½®ï¼‰ï¼Œå¼ºåˆ¶æ”¹ä¸º .jpg
        root, ext = os.path.splitext(save_path)
        if ext.lower() not in ['.jpg', '.jpeg', '.png']:
            save_path = root + ".jpg"
        
        cv2.imwrite(save_path, img)
        print(f"âœ… å›¾ç‰‡æ¨ç†å®Œæˆï¼Œå·²ä¿å­˜è‡³: {save_path}")

        # 5. ç”Ÿæˆåˆ†ææŠ¥å‘Š (å¦‚æœä¼ å…¥äº† report_path)
        if report_path:
            self._generate_report(report_path)

    def _process_video(self, vid_path, save_path, report_path):
        cap = cv2.VideoCapture(vid_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        w, h = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        out = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))
        
        frame_idx = 0
        print(f"ğŸ¥ [è§†é¢‘] å¼€å§‹æ¨ç†ä¸æ•°æ®æŒ–æ˜...")

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret: break
            
            # ä½¿ç”¨è·Ÿè¸ªæ¨¡å¼
            # results = self.model.track(frame, persist=True, conf=0.25, verbose=False)[0]
            results = self.model.predict(frame, conf=0.25, verbose=False)[0]
            
            # 1. å®æ—¶ç»Ÿè®¡ ID
            if results.boxes.id is not None:
                track_ids = results.boxes.id.int().cpu().numpy()
                cls_ids = results.boxes.cls.int().cpu().numpy()
                for t_id, c_id in zip(track_ids, cls_ids):
                    self.vehicle_counts[self.class_names[c_id]].add(t_id)

            # 2. é‡‡æ ·ï¼šæ¯ç§’è®°å½•ä¸€æ¬¡è½¦æµé‡æ•°æ® (ç”¨äºæŠ˜çº¿å›¾)
            if frame_idx % int(fps) == 0:
                current_total = sum(len(ids) for ids in self.vehicle_counts.values())
                timestamp = frame_idx / fps
                self.time_series_data.append({'time': timestamp, 'count': current_total})

            # 3. ç»˜å›¾ä¸é¢æ¿æ˜¾ç¤º
            self._draw_results(frame, results, is_video=True)
            self._draw_statistics_panel(frame)
            
            out.write(frame)
            frame_idx += 1
            if frame_idx % 30 == 0: print(f"Processing... {frame_idx} frames", end='\r')

        cap.release()
        out.release()
        
        # ğŸš€ ä»»åŠ¡ç»“æŸï¼šç”ŸæˆäºŒæ¬¡æ•°æ®æŒ–æ˜æŠ¥å‘Š
        self._generate_report(report_path)

    def _draw_results(self, img, results, is_video=False):
        if results.boxes:
            boxes = results.boxes.xyxy.cpu().numpy()
            clses = results.boxes.cls.int().cpu().numpy()
            confs = results.boxes.conf.cpu().numpy()
            ids = results.boxes.id.int().cpu().numpy() if (is_video and results.boxes.id is not None) else None
            
            for i, box in enumerate(boxes):
                x1, y1, x2, y2 = map(int, box)
                class_name = self.class_names[clses[i]]
                color = COLORS.get(class_name, DEFAULT_COLOR)
                
                label = f"{class_name} {confs[i]:.2f}"
                if ids is not None: label = f"ID:{ids[i]} " + label
                
                cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
                # ç®€å•ç”»æ–‡å­—èƒŒæ™¯
                (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
                cv2.rectangle(img, (x1, y1-th-5), (x1+tw, y1), color, -1)
                cv2.putText(img, label, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)

    def _draw_statistics_panel(self, img):
        """å®æ—¶ HUD é¢æ¿"""
        overlay = img.copy()
        cv2.rectangle(overlay, (10, 10), (220, 180), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.5, img, 0.5, 0, img)
        cv2.putText(img, "Real-time Traffic", (20, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)
        y = 65
        for cls in sorted(self.vehicle_counts.keys()):
            count = len(self.vehicle_counts[cls])
            cv2.putText(img, f"{cls}: {count}", (20, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[cls], 1)
            y += 20

    def _generate_report(self, report_path):
        """æ ¸å¿ƒï¼šäºŒæ¬¡æ•°æ®æŒ–æ˜å¯è§†åŒ–æŠ¥å‘Šç”Ÿæˆ"""
        print(f"\nğŸ“Š æ­£åœ¨ç”Ÿæˆæ•°æ®æŒ–æ˜æŠ¥å‘Š: {report_path}")
        
        # å‡†å¤‡æ•°æ®
        cls_data = {cls: len(ids) for cls, ids in self.vehicle_counts.items()}
        df_time = pd.DataFrame(self.time_series_data)

        # åˆ›å»ºç”»å¸ƒ (åŒ…å«ä¸¤ä¸ªå­å›¾)
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        plt.rcParams['font.sans-serif'] = ['SimHei'] # è§£å†³ä¸­æ–‡ä¹±ç 
        
        # --- å›¾ 1ï¼šè½¦å‹åˆ†å¸ƒé¥¼å›¾ ---
        if cls_data:
            labels = list(cls_data.keys())
            sizes = list(cls_data.values())
            # å°† OpenCV BGR è½¬æ¢ä¸º Matplotlib RGB
            pie_colors = [tuple(reversed([c/255 for c in COLORS[l]])) for l in labels]
            ax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=pie_colors, shadow=True)
            ax1.set_title("å„è½¦å‹è¯†åˆ«åˆ†å¸ƒæ¯”ä¾‹", fontsize=14)

        # --- å›¾ 2ï¼šè½¦æµé‡éšæ—¶é—´å˜åŒ–æŠ˜çº¿å›¾ ---
        if not df_time.empty:
            ax2.plot(df_time['time'], df_time['count'], marker='o', linestyle='-', color='b', linewidth=2)
            ax2.fill_between(df_time['time'], df_time['count'], color='skyblue', alpha=0.3)
            ax2.set_xlabel("æ—¶é—´ (s)", fontsize=12)
            ax2.set_ylabel("ç´¯è®¡æ£€æµ‹æ•°é‡ (å°)", fontsize=12)
            ax2.set_title("è½¦æµé‡éšæ—¶é—´å¢é•¿è¶‹åŠ¿", fontsize=14)
            ax2.grid(True, linestyle='--', alpha=0.7)

        plt.suptitle(f"BIT_CLS æ•°æ®é›†ç³»ç»Ÿæ¨ç†åˆ†ææŠ¥å‘Š\n(Total Vehicles: {sum(cls_data.values())})", fontsize=18)
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        
        # ä¿å­˜å›¾è¡¨
        plt.savefig(report_path)
        print(f"âœ¨ æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")

# ================= ğŸš€ ä¸»ç¨‹åº =================
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--input', type=str, required=True, help="è¾“å…¥è·¯å¾„")
    parser.add_argument('--model', default='./task1/best.pt')
    parser.add_argument('--out_dir', default='runs')
    args = parser.parse_args()

    os.makedirs(args.out_dir, exist_ok=True)
    filename = os.path.basename(args.input).split('.')[0]
    
    vid_out = os.path.join(args.out_dir, f"result_{filename}.mp4")
    report_out = os.path.join(args.out_dir, f"report_{filename}.png")

    engine = InferenceEngine(model_path=args.model)
    engine.run(args.input, vid_out, report_out)