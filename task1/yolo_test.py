import os
import time
import torch
import pandas as pd
from ultralytics import YOLO
import shutil

# ================= ğŸ”§ é…ç½®åŒºåŸŸ =================
# 1. å¾®è°ƒåçš„ YOLO æ¨¡å‹è·¯å¾„
MODEL_PATH = '/data2/zhuangyn/Deep-Learning-coding-task/task1/code/best.pt'
# 2. ä½ çš„ data.yaml è·¯å¾„ (ç¡®ä¿é‡Œé¢ test: è·¯å¾„æŒ‡å‘äº†æ­£ç¡®çš„æµ‹è¯•é›†)
DATA_YAML_PATH = '/data2/zhuangyn/Deep-Learning-coding-task/task1/dataset/BIT_YOLO_Dataset/data.yaml'
# 3. ç»“æœè¾“å‡ºç›®å½•
OUTPUT_ROOT = "runs/scheme_a_yolo_benchmark"
# ===============================================

def run_eval():
    # å‡†å¤‡ç›®å½•
    if os.path.exists(OUTPUT_ROOT):
        shutil.rmtree(OUTPUT_ROOT)
    os.makedirs(OUTPUT_ROOT)

    # 1. åŠ è½½æ¨¡å‹
    print(f"ğŸ”¥ æ­£åœ¨åŠ è½½å¾®è°ƒæ¨¡å‹: {MODEL_PATH}")
    model = YOLO(MODEL_PATH)

    # 2. æ‰§è¡Œè¯„æµ‹ (Validation mode on Test split)
    # split='test' ä¼šè®©æ¨¡å‹å»è¯» data.yaml ä¸­ test è·¯å¾„ä¸‹çš„æ•°æ®
    print(f"ğŸ§ª æ­£åœ¨æµ‹è¯•é›†ä¸Šæ‰§è¡Œå…¨é‡è¯„ä¼°...")
    
    # è®¡æ—¶å¼€å§‹
    t_start = time.time()
    
    # model.val ä¼šè‡ªåŠ¨è®¡ç®— mAP, Precision, Recall ç­‰
    results = model.val(
        data=DATA_YAML_PATH,
        split='test',      # æŒ‡å®šä½¿ç”¨ test é›†
        imgsz=640,         # ä¿æŒä¸è®­ç»ƒä¸€è‡´
        conf=0.25,         # ç½®ä¿¡åº¦é˜ˆå€¼
        iou=0.6,           # NMS IoU é˜ˆå€¼
        device=0,          # æŒ‡å®š GPU ID
        save_json=True,    # ä¿å­˜ç»“æœ json
        project=OUTPUT_ROOT,
        name='test_results'
    )
    
    t_end = time.time()

    # 3. æå–æ ¸å¿ƒæŒ‡æ ‡
    # metrics åŒ…å«å¤šç§ç²¾åº¦æ•°æ®
    metrics_dict = {
        "Model": "YOLO11_Scheme_A",
        "mAP50": results.box.map50,           # mAP at IoU=0.5
        "mAP50-95": results.box.map,         # mAP at IoU=0.5:0.95
        "Precision": results.box.mp,          # Mean Precision
        "Recall": results.box.mr,             # Mean Recall
        "Fitness": results.fitness            # ç»¼åˆè¯„ä»·æŒ‡æ ‡
    }

    # 4. é€Ÿåº¦æµ‹è¯• (Inference Speed)
    # åˆ©ç”¨ val å†…éƒ¨è®°å½•çš„æ—¶é—´
    speed_info = results.speed # å­—å…¸æ ¼å¼ {'preprocess': ms, 'inference': ms, 'loss': ms, 'postprocess': ms}
    total_latency_ms = speed_info['preprocess'] + speed_info['inference'] + speed_info['postprocess']
    fps = 1000 / total_latency_ms

    metrics_dict["Latency_ms"] = total_latency_ms
    metrics_dict["FPS"] = fps

    # 5. ä¿å­˜å¹¶æ‰“å°æŠ¥è¡¨
    df = pd.DataFrame([metrics_dict])
    print("\n" + "="*60)
    print("ğŸ† æ–¹æ¡ˆ A (ç«¯åˆ°ç«¯ YOLO) æµ‹è¯•é›†è¯„æµ‹æŠ¥å‘Š")
    print("="*60)
    print(df.round(4).to_string(index=False))
    
    report_csv = os.path.join(OUTPUT_ROOT, "scheme_a_summary.csv")
    df.to_csv(report_csv, index=False)
    
    print(f"\nâœ… è¯¦ç»†æŒ‡æ ‡å·²ä¿å­˜è‡³: {report_csv}")
    print(f"ğŸ–¼ï¸  æ£€æµ‹å¯è§†åŒ–å›¾ç‰‡å·²ä¿å­˜è‡³: {OUTPUT_ROOT}/test_results/")

    # 6. å•ç‹¬è¾“å‡ºæ¯ä¸€ç±»çš„æŒ‡æ ‡ (æ–¹ä¾¿åˆ†æ Sedan æ•ˆæœ)
    print("\nğŸ“Š é€ç±»åˆ«è¯¦ç»†æŒ‡æ ‡:")
    class_names = model.names
    # results.box.p/r/ap åˆ†åˆ«æ˜¯æ¯ä¸€ç±»çš„ P, R, AP
    class_data = []
    for i, name in class_names.items():
        class_data.append({
            "Class": name,
            "Precision": results.box.p[i],
            "Recall": results.box.r[i],
            "AP50": results.box.ap50[i]
        })
    df_class = pd.DataFrame(class_data)
    print(df_class.round(4).to_string(index=False))
    df_class.to_csv(os.path.join(OUTPUT_ROOT, "scheme_a_class_metrics.csv"), index=False)

if __name__ == "__main__":
    run_eval()